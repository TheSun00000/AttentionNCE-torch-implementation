{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from evaluation import knn_evaluation\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SimCLRDataset(Dataset):\n",
    "#     def __init__(self, dataset, transform, num_positives):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             dataset: Base dataset (e.g., CIFAR-10)\n",
    "#             transform: Transformation pipeline for data augmentation\n",
    "#             num_positives: Number of positive samples to generate per anchor\n",
    "#         \"\"\"\n",
    "#         self.dataset = dataset\n",
    "#         self.transform = transform\n",
    "#         self.num_positives = num_positives\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # Get the anchor image and its label\n",
    "#         img, label = self.dataset[idx]\n",
    "        \n",
    "#         anchor = self.transform(img)\n",
    "#         # Apply transformations to generate positive samples\n",
    "#         positives = [self.transform(img) for _ in range(self.num_positives)]\n",
    "\n",
    "#         return anchor, torch.stack(positives), label\n",
    "    \n",
    "    \n",
    "\n",
    "class SimCLRDataset(Dataset):\n",
    "    def __init__(self, dataset, transform, num_positives):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: Base dataset (e.g., CIFAR-10)\n",
    "            transform: Transformation pipeline for data augmentation\n",
    "            num_positives: Number of positive samples to generate per anchor\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.num_positives = num_positives\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the anchor image and its label\n",
    "        img, label = self.dataset[idx]\n",
    "        \n",
    "        anchor1 = self.transform(img)\n",
    "        anchor2 = self.transform(img)\n",
    "\n",
    "        # Apply transformations to generate positive samples\n",
    "        positives = [self.transform(img) for _ in range(self.num_positives)]\n",
    "\n",
    "        return [anchor1, anchor2], positives, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 dataset\n",
    "cifar10_train = datasets.CIFAR10(\n",
    "    root=\"./dataset/cifar10\",\n",
    "    train=True,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "# Transformation pipeline for data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=32),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18Encoder(nn.Module):\n",
    "    def __init__(self, projection_dim):\n",
    "        super(ResNet18Encoder, self).__init__()\n",
    "        # Load pre-trained ResNet18 and remove the fully connected layer\n",
    "        self.encoder = models.resnet18(weights=None)\n",
    "        self.encoder = nn.Sequential(*list(self.encoder.children())[:-1])\n",
    "        \n",
    "        # Add a projection head\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through ResNet18 encoder\n",
    "        features = self.encoder(x)\n",
    "        features = torch.flatten(features, start_dim=1)  # Flatten the output\n",
    "        # Forward pass through the projection head\n",
    "        projections = self.projection_head(features)\n",
    "        \n",
    "        # projections = projections / projections.norm(dim=0)\n",
    "        projections = nn.functional.normalize(projections, p=2, dim=1)\n",
    "        \n",
    "        return projections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_mask(batch_size):\n",
    "    negative_mask = torch.ones((batch_size, 2 * batch_size), dtype=bool)\n",
    "    for i in range(batch_size):\n",
    "        negative_mask[i, i] = 0\n",
    "        negative_mask[i, i + batch_size] = 0\n",
    "\n",
    "    negative_mask = torch.cat((negative_mask, negative_mask), 0)\n",
    "    return negative_mask\n",
    "\n",
    "\n",
    "\n",
    "def AttentioNCE(out_1, out_2, out_pos, batch_size, temperature=0.5, d_pos=1., d_neg=1.):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    # anchor points\n",
    "    out_1 : anchor features [bs,dim]\n",
    "    out_2 : anchor features [bs,dim]\n",
    "\n",
    "    # 4 positive views for  each anchor point \n",
    "    out_3 : positive view1 features [bs,dim]\n",
    "    out_4 : positive view2 features [bs,dim]\n",
    "    out_5 : positive view3 features [bs,dim]\n",
    "    out_6 : positive view4 features [bs,dim]\n",
    "\n",
    "    batch_size : Under SimCLR framework, negative sample size N = 2 x (batch size - 1)\n",
    "    temperature: temperature scaling\n",
    "    d_pos : positive scaling factor\n",
    "    d_neg : negative scaling factor\n",
    "    Returns: AttentioNCE loss values [2bs,]\n",
    "    -------\n",
    "    '''\n",
    "    \n",
    "    feature_dim = 128\n",
    "    num_pos = len(out_pos)\n",
    "    \n",
    "    anchor = torch.cat([out_1, out_2], dim=0)  # [2bs*dim]\n",
    "\n",
    "    # pos score\n",
    "    pos_views = torch.cat(out_pos,dim=1).view(-1, num_pos, feature_dim) # [bs,num_pos,dim]\n",
    "    pos_sim = torch.sum(anchor.view(2, batch_size, 1, feature_dim) * pos_views, dim=-1).view(-1, num_pos)\n",
    "    # [2,bs,1,dim] x [bs,num_pos,dim]  -> [2,bs,num_pos,dim] ->  [2,bs,num_pos]-> [2bs,num_pos]\n",
    "    alpha = torch.nn.functional.softmax(pos_sim.detach()/ d_pos, dim=-1) # [2bs,num_pos]\n",
    "    pos_score = torch.exp(torch.sum(pos_sim * alpha, dim=-1) / temperature)  # [2bs,] This step is obtained based on the additivity property of inner product operations.\n",
    "\n",
    "    # neg score\n",
    "    sim = torch.mm(anchor, anchor.t().contiguous())  # [2bs,2bs]\n",
    "    mask = get_negative_mask(batch_size).to(device)\n",
    "    neg_sim = sim.masked_select(mask).view(2 * batch_size, -1)  # [2bs, 2bs-2]\n",
    "    beta = (torch.nn.functional.softmax(neg_sim.detach() / d_neg, dim=-1) + 1e-6) * (2 * batch_size - 2)\n",
    "    neg_score = torch.exp(neg_sim * beta / temperature)  # [2bs, 2N-2] This step is obtained based on the additivity property of inner product operations.\n",
    "\n",
    "    # contrastive loss\n",
    "    loss = - torch.log(pos_score / (pos_score + neg_score.sum(dim=-1))).mean() # [2bs,]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos = 5\n",
    "projection_dim = 128\n",
    "n_anchors = 256\n",
    "epochs = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "simclr_dataset = SimCLRDataset(cifar10_train, transform, n_pos)\n",
    "data_loader = DataLoader(simclr_dataset, batch_size=n_anchors, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ResNet18Encoder(projection_dim).to(device)\n",
    "# optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n",
    "# criterion = AttentionNCE(\n",
    "#     n_anchors=n_anchors,\n",
    "#     dim=projection_dim,\n",
    "#     n_pos=n_pos,\n",
    "#     temperature=1.\n",
    "# )\n",
    "\n",
    "losses = []\n",
    "accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   6%|▌         | 11/195 [00:06<01:52,  1.63it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n_step, (anchors, positives, labels) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(data_loader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(data_loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# for iter, (anchors, positives, labels) in enumerate(data_loader):\u001b[39;00m\n\u001b[1;32m      4\u001b[0m         \n\u001b[1;32m      5\u001b[0m         \n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# x = torch.concat((anchor.unsqueeze(1), positives), dim=1)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# x = x.reshape(n_anchors + n_anchors*n_pos, 3, 32, 32)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         anchor1, anchor2 \u001b[38;5;241m=\u001b[39m anchors\n\u001b[1;32m     11\u001b[0m         anchor1 \u001b[38;5;241m=\u001b[39m anchor1\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[3], line 51\u001b[0m, in \u001b[0;36mSimCLRDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     48\u001b[0m anchor2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Apply transformations to generate positive samples\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m positives \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_positives)]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [anchor1, anchor2], positives, label\n",
      "Cell \u001b[0;32mIn[3], line 51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     48\u001b[0m anchor2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Apply transformations to generate positive samples\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m positives \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_positives)]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [anchor1, anchor2], positives, label\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/transforms.py:540\u001b[0m, in \u001b[0;36mRandomApply.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m--> 540\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/transforms.py:1280\u001b[0m, in \u001b[0;36mColorJitter.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1278\u001b[0m         img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madjust_saturation(img, saturation_factor)\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m fn_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hue_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1280\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_hue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:968\u001b[0m, in \u001b[0;36madjust_hue\u001b[0;34m(img, hue_factor)\u001b[0m\n\u001b[1;32m    966\u001b[0m     _log_api_usage_once(adjust_hue)\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_hue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39madjust_hue(img, hue_factor)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py:117\u001b[0m, in \u001b[0;36madjust_hue\u001b[0;34m(img, hue_factor)\u001b[0m\n\u001b[1;32m    113\u001b[0m np_h \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(hue_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m    115\u001b[0m h \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(np_h, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 117\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHSV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/PIL/Image.py:1146\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m   1143\u001b[0m     dither \u001b[38;5;241m=\u001b[39m Dither\u001b[38;5;241m.\u001b[39mFLOYDSTEINBERG\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1146\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdither\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m         \u001b[38;5;66;03m# normalize source image and try again\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(epochs):\n",
    "    for n_step, (anchors, positives, labels) in tqdm(enumerate(data_loader), total=len(data_loader), desc=f'Epoch {epoch}'):\n",
    "    # for iter, (anchors, positives, labels) in enumerate(data_loader):\n",
    "        \n",
    "        \n",
    "        # x = torch.concat((anchor.unsqueeze(1), positives), dim=1)\n",
    "        # x = x.reshape(n_anchors + n_anchors*n_pos, 3, 32, 32)\n",
    "        \n",
    "        anchor1, anchor2 = anchors\n",
    "        \n",
    "        anchor1 = anchor1.to(device)\n",
    "        anchor2 = anchor2.to(device)\n",
    "        \n",
    "        out_1 = encoder(anchor1)\n",
    "        out_2 = encoder(anchor2)\n",
    "        out_pos = []\n",
    "        for pos in positives:\n",
    "            out = encoder(pos.to(device))\n",
    "            out_pos.append(out)\n",
    "        \n",
    "        loss = AttentioNCE(out_1, out_2, out_pos, batch_size=n_anchors)\n",
    "        # loss, pos_terms, neg_terms, scores = criterion(q, k)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        \n",
    "        # print(loss)\n",
    "        # print()\n",
    "        \n",
    "        if n_step % (len(data_loader)//10) == 0 or n_step == len(data_loader)-1:\n",
    "            clear_output(True)\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            axs[0].plot(losses)\n",
    "            axs[0].set_title('Training Loss', fontsize=14)\n",
    "            axs[1].plot(accs)\n",
    "            axs[1].set_title('Accuracy', fontsize=14)\n",
    "            plt.show()\n",
    "        \n",
    "        # break\n",
    "        \n",
    "    acc = knn_evaluation(encoder, dataset='cifar10')\n",
    "    accs.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
