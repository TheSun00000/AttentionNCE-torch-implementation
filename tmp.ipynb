{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionNCE(nn.Module):\n",
    "    def __init__(self, n_anchors, dim, n_pos, d_pos=1., d_neg=1., temperature=0.5):\n",
    "        super(AttentionNCE, self).__init__()\n",
    "        self.n_anchors = n_anchors\n",
    "        self.dim = dim\n",
    "        self.n_pos = n_pos\n",
    "        self.d_pos = d_pos\n",
    "        self.d_neg = d_neg\n",
    "        self.temperature = temperature\n",
    "        \n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor) -> torch.Tensor:\n",
    "        assert q.shape == (self.n_anchors, self.dim)\n",
    "        assert k.shape == (self.n_anchors, self.n_pos, self.dim)\n",
    "        \n",
    "        n_anchors = self.n_anchors\n",
    "        n_pos = self.n_pos\n",
    "        dim = self.dim\n",
    "        \n",
    "        \n",
    "        scores = torch.matmul(q, k.reshape(-1, dim).t()) # (n_anchors, n_anchors*n_pos)\n",
    "        # print(scores.mean())\n",
    "\n",
    "        pos_mask = torch.zeros((n_anchors, n_anchors*n_pos), dtype=torch.bool, device=device)\n",
    "        for i in range(n_anchors):\n",
    "            pos_mask[i, i*n_pos:(i+1)*n_pos] = 1.\n",
    "        neg_mask = ~pos_mask\n",
    "\n",
    "        pos_scores = scores[pos_mask].reshape(n_anchors, n_pos)    # (n_anchors, n_pos)\n",
    "        neg_scores = scores[neg_mask].reshape(n_anchors, n_anchors*n_pos - n_pos) # (n_anchors, n_anchors*n_pos - n_pos)\n",
    "\n",
    "\n",
    "        pos_scores = torch.softmax(pos_scores / self.d_pos, dim=-1) # (n_anchors, n_pos)\n",
    "        neg_scores = torch.softmax(neg_scores / self.d_neg, dim=-1) * n_anchors # (n_anchors, n_anchors*n_pos - n_pos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        h_pos = (pos_scores.unsqueeze(-1) * k).sum(-2) # (n_anchors, dim)\n",
    "        \n",
    "        # (n_anchors, dim) = (n_anchors, dim) * (n_anchors, dim)\n",
    "        res = (q * h_pos)\n",
    "        res = res.sum(-1) # (n_anchors,)\n",
    "        pos_terms = torch.exp(res / self.temperature) # (n_anchors,)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        neg_terms = torch.zeros((n_anchors, ), dtype=torch.float32, device=device)\n",
    "\n",
    "        for i in range(n_anchors):\n",
    "            # Take all the negative samples compared to the i-th query\n",
    "            neg_k = torch.concat((k[:i], k[i+1:]), dim=0).reshape(-1, dim) # (n_anchors*n_pos - n_pos, dim)\n",
    "            \n",
    "            # Mutiply the negative scores with the negative keys\n",
    "            \n",
    "            # (n_anchors*n_pos - n_pos, dim) = (n_anchors*n_pos - n_pos) * (n_anchors*n_pos - n_pos, dim)\n",
    "            n = neg_scores[i].unsqueeze(-1) * neg_k\n",
    "            \n",
    "            # (n_anchors*n_pos - n_pos) = (n_anchors*n_pos - n_pos, dim) * (dim)\n",
    "            res = torch.matmul(n, q[i])\n",
    "            \n",
    "            res = torch.exp(res / self.temperature) # (n_anchors*n_pos - n_pos)\n",
    "            res = res.sum() # (1)\n",
    "            \n",
    "            # Store the negative term of the i-th query\n",
    "            neg_terms[i] = res\n",
    "            \n",
    "            \n",
    "        # print(pos_terms / (pos_terms + neg_terms))\n",
    "        # attentionNCE_loss = - torch.log( pos_terms / (pos_terms + neg_terms) ).mean()\n",
    "        attentionNCE_loss = - torch.log( pos_terms / (pos_terms + neg_terms) ).mean()\n",
    "        return attentionNCE_loss, pos_terms, neg_terms, scores\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
